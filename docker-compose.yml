version: '3.8'

services:
  namenode:
    image: apachehudi/hudi-hadoop_2.8.4-namenode:linux-arm64-0.10.1
    platform: linux/arm64
    hostname: namenode
    container_name: namenode
    environment:
      - CLUSTER_NAME=hudi_hadoop284_hive232_spark244
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    ports:
      - "50070:50070"
      - "8020:8020"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://namenode:50070"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - namenode:/hadoop/dfs/name
    networks:
      - hadoop_net

  datanode:
    image: apachehudi/hudi-hadoop_2.8.4-datanode:linux-arm64-0.10.1
    platform: linux/arm64
    container_name: datanode
    hostname: datanode
    environment:
      - CLUSTER_NAME=hudi_hadoop284_hive232_spark244
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    ports:
      - "50075:50075"
    depends_on:
      - namenode
    healthcheck:
      test: ["CMD", "curl", "-f", "http://datanode:50075"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - datanode:/hadoop/dfs/data
    networks:
      - hadoop_net

  hive-metastore-postgresql:
    image: menorah84/hive-metastore-postgresql:2.3.0
    platform: linux/arm64
    environment:
      POSTGRES_HOST_AUTH_METHOD: trust
    volumes:
      - hive-metastore-postgresql:/var/lib/postgresql
    hostname: hive-metastore-postgresql
    container_name: hive-metastore-postgresql
    networks:
      - hadoop_net

  hivemetastore:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3:linux-arm64-0.10.1
    platform: linux/arm64
    hostname: hivemetastore
    container_name: hivemetastore
    environment:
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore-postgresql/metastore
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=org.postgresql.Driver
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=hive
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword=hive
      - HIVE_SITE_CONF_datanucleus_autoCreateSchema=false
      - HIVE_SITE_CONF_hive_metastore_uris=thrift://hivemetastore:9083
      - SERVICE_PRECONDITION=namenode:50070 hive-metastore-postgresql:5432
    command: /opt/hive/bin/hive --service metastore
    ports:
      - "9083:9083"
    depends_on:
      - hive-metastore-postgresql
      - namenode
    healthcheck:
      test: ["CMD", "nc", "-z", "hivemetastore", "9083"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - hadoop_net

  hiveserver:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3:linux-arm64-0.10.1
    platform: linux/arm64
    hostname: hiveserver
    container_name: hiveserver
    environment:
      - HIVE_SITE_CONF_hive_metastore_uris=thrift://hivemetastore:9083
      - SERVICE_PRECONDITION=hivemetastore:9083
    ports:
      - "10000:10000"
    depends_on:
      - hivemetastore
    networks:
      - hadoop_net

  sparkmaster:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3-sparkmaster_2.4.4:linux-arm64-0.10.1
    platform: linux/arm64
    hostname: sparkmaster
    container_name: sparkmaster
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - SPARK_MASTER_HOST=sparkmaster
      - INIT_DAEMON_STEP=setup_spark
    ports:
      - "8080:8080"
      - "7077:7077"
    depends_on:
      - hivemetastore
      - hiveserver
    networks:
      - hadoop_net

  spark-worker-1:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3-sparkworker_2.4.4:linux-arm64-0.10.1
    platform: linux/arm64
    hostname: spark-worker-1
    container_name: spark-worker-1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - SPARK_MASTER=spark://sparkmaster:7077
    depends_on:
      - sparkmaster
    ports:
      - "8081:8081"
    networks:
      - hadoop_net

  zookeeper:
    image: arm64v8/zookeeper:3.4.12
    platform: linux/arm64
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - hadoop_net

  kafka:
    image: wurstmeister/kafka:2.12-2.0.1
    platform: linux/arm64
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_ADVERTISED_HOST_NAME=kafka
    depends_on:
      - zookeeper
    networks:
      - hadoop_net

  spark-submit:
    image: apachehudi/hudi-hadoop_2.8.4-hive_2.3.3-sparkadhoc_2.4.4:linux-arm64-0.10.1
    platform: linux/arm64
    hostname: spark-submit
    container_name: spark-submit
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - SPARK_MASTER=spark://sparkmaster:7077
    depends_on:
      - sparkmaster
      - kafka  # Updated to reference "kafka" instead of "kafkabroker"
    command: bash -c "spark-submit --master spark://sparkmaster:7077 /app/scripts/spark_batch_processing.py"
    volumes:
      - .:/app
      - ./data:/app/data
    networks:
      - hadoop_net

  app:
    build: .
    platform: linux/arm64
    hostname: app
    environment:
      - WEATHER_API_KEY=${WEATHER_API_KEY}
    depends_on:
      - kafka  # Updated to reference "kafka" instead of "kafkabroker"
    command: python scripts/collect_data.py
    volumes:
      - ./data:/app/data
    networks:
      - hadoop_net

volumes:
  namenode:
  datanode:
  hive-metastore-postgresql:

networks:
  hadoop_net:
    driver: bridge
